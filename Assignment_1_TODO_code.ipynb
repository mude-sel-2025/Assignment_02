{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "048b7675",
      "metadata": {
        "id": "048b7675"
      },
      "source": [
        "# Assignment 1 — Numerical Modelling (Python)\n",
        "\n",
        "\n",
        "**Function**: \\( f(x)=\\sin x \\), **exact derivative**: \\( f'(x)=\\cos x \\)\n",
        "\n",
        "This notebook provides:\n",
        "1. Finite differences at selected points (manual + code); compare errors.\n",
        "2. Error vs. step-size (log–log) to check convergence orders.\n",
        "3. Full domain \\((0,2\\pi)\\): discretize, compute derivatives (F/B/C), compare with exact; report error stats.\n",
        "4. Newton–Raphson roots of \\(\\sin x\\) on \\((0,4\\pi)\\) using exact and finite-difference derivatives.\n",
        "\n",
        "TODO Task for the Students:\n",
        "\n",
        "Write the code in the cell wherever TODO is mentioned and run the code to check the results and verify by the TAs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Required Libraries\n",
        "# -------------------------\n",
        "import math, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "mc40zTlRJizb"
      },
      "id": "mc40zTlRJizb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baced456",
      "metadata": {
        "id": "baced456"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Editable parameters\n",
        "# -------------------------\n",
        "X_POINTS = [math.pi/4, math.pi/2, math.pi]\n",
        "H_VALUES = [1e-1, 5e-2, 2e-2, 1e-2, 5e-3, 2e-3, 1e-3]   # step sizes for convergence study\n",
        "H_DOMAIN = 0.02     # step for domain (0, 2π);\n",
        "INITIAL_GUESSES = [0.1, 1.0, 3.0, 5.0, 7.0, 10.0, 12.0]  # in (0, 4π)\n",
        "H_NEWTON = 1e-4     # step used by FD-Newton;\n",
        "\n",
        "def f(x): return ## TODO define the exact function\n",
        "def df_exact(x): return ## TODO define the derivative of the exact function\n",
        "def d_forward(f, x, h):  return (f(x + h) - f(x)) / h\n",
        "def d_backward(f, x, h): ## TODO write the expression for the backward finite difference method\n",
        "def d_central(f, x, h):  ## TODO write the expression for the central finite difference method\n",
        "\n",
        "def abs_err(approx, exact): return abs(approx - exact)\n",
        "\n",
        "def evaluate_at_points(points, h):\n",
        "    rows = []\n",
        "    for x in points:\n",
        "        exact = df_exact(x)\n",
        "        fwd = d_forward(f, x, h)\n",
        "        bwd = ## TODO call the backward finite difference method function\n",
        "        cen = ## TODO call the central finite difference method function\n",
        "        rows.append({\n",
        "            \"x\": x, \"h\": h, \"exact\": exact,\n",
        "            \"forward\": fwd, \"err_forward\": abs_err(fwd, exact),\n",
        "            \"backward\": bwd, \"err_backward\": abs_err(bwd, exact),\n",
        "            \"central\": cen, \"err_central\": abs_err(cen, exact),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def convergence_study(points, h_values):\n",
        "    hs, ef, eb, ec = [], [], [], []\n",
        "    for h in h_values:\n",
        "        batch = evaluate_at_points(points, h)\n",
        "        ef.append(np.mean([r[\"err_forward\"]  for r in batch]))\n",
        "        eb.append(np.mean([r[\"err_backward\"] for r in batch]))\n",
        "        ec.append(np.mean([r[\"err_central\"]  for r in batch]))\n",
        "        hs.append(h)\n",
        "    return {\"h\": np.array(hs), \"err_forward\": np.array(ef),\n",
        "            \"err_backward\": np.array(eb), \"err_central\": np.array(ec)}\n",
        "\n",
        "def slope_loglog(x, y):\n",
        "    lx, ly = np.log10(x), np.log10(y)\n",
        "    m, c = np.polyfit(lx, ly, 1)\n",
        "    return m, c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14606519",
      "metadata": {
        "id": "14606519"
      },
      "source": [
        "## 1) Selected points — compute FD derivatives and errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a247f5e",
      "metadata": {
        "id": "5a247f5e"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------------\n",
        "# Tabular representation of the 1st Objective of the Assignment\n",
        "# --------------------------------------------------------------\n",
        "h = H_VALUES[0]\n",
        "rows = evaluate_at_points(X_POINTS, h)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(rows)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# Tabular representation of the 2nd Objective of the Assignment\n",
        "# --------------------------------------------------------------\n",
        "res = convergence_study(X_POINTS, H_VALUES)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(res)\n",
        "df"
      ],
      "metadata": {
        "id": "raZIoTHdq7nM"
      },
      "id": "raZIoTHdq7nM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6f55ae06",
      "metadata": {
        "id": "6f55ae06"
      },
      "source": [
        "## 2) Convergence — error vs. h (log–log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ffcd55",
      "metadata": {
        "id": "41ffcd55"
      },
      "outputs": [],
      "source": [
        "res = convergence_study(X_POINTS, H_VALUES)\n",
        "m_f, _ = slope_loglog(res[\"h\"], res[\"err_forward\"])\n",
        "m_b, _ = slope_loglog(res[\"h\"], res[\"err_backward\"])\n",
        "m_c, _ = slope_loglog(res[\"h\"], res[\"err_central\"])\n",
        "print(f\"Estimated orders: forward≈{m_f:.2f}, backward≈{m_b:.2f}, central≈{m_c:.2f}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.loglog(res[\"h\"], res[\"err_forward\"], marker=\"o\", linestyle=\"--\", label=\"Forward\")\n",
        "plt.loglog(res[\"h\"], res[\"err_backward\"], marker=\"s\", linestyle=\"--\", label=\"Backward\")\n",
        "plt.loglog(res[\"h\"], res[\"err_central\"], marker=\"^\", linestyle=\"--\", label=\"Central\")\n",
        "plt.gca().invert_xaxis()\n",
        "plt.xlabel(\"h\"); plt.ylabel(\"Mean abs error\")\n",
        "plt.title(\"Finite-difference error vs step size (log–log)\")\n",
        "plt.legend(); plt.tight_layout()\n",
        "plt.savefig(\"fd_convergence.jpg\", dpi=160)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f1928c",
      "metadata": {
        "id": "01f1928c"
      },
      "source": [
        "## 3) Full domain (0, 2π): discretize and compare with exact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d5e7a9",
      "metadata": {
        "id": "86d5e7a9"
      },
      "outputs": [],
      "source": [
        "def grid_over_domain(a, b, h):\n",
        "    n = int(math.floor((b - a) / h)) + 1\n",
        "    xs = a + np.arange(n)*h\n",
        "    xs = xs[xs <= b]\n",
        "    return xs\n",
        "\n",
        "def evaluate_on_domain(h):\n",
        "    a, b = 0.0, 2.0*math.pi\n",
        "    xs = grid_over_domain(a, b, h)\n",
        "    fwd = np.zeros_like(xs); bwd = np.zeros_like(xs); cen = np.zeros_like(xs)\n",
        "    exact = np.cos(xs)\n",
        "    for i, x in enumerate(xs):\n",
        "        if i == 0:\n",
        "            fwd[i] = '''TODO'''; bwd[i] = '''TODO'''; cen[i] = '''TODO'''\n",
        "        elif i == len(xs)-1:\n",
        "            fwd[i] = '''TODO'''; bwd[i] = '''TODO'''; cen[i] = '''TODO'''\n",
        "        else:\n",
        "            fwd[i] = '''TODO'''; bwd[i] = '''TODO'''; cen[i] = '''TODO'''\n",
        "    ef = np.abs(fwd - exact); eb = np.abs(bwd - exact); ec = np.abs(cen - exact)\n",
        "    return xs, exact, fwd, bwd, cen, ef, eb, ec\n",
        "\n",
        "xs, exact, fwd, bwd, cen, ef, eb, ec = evaluate_on_domain(H_DOMAIN)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 8), constrained_layout=True)\n",
        "\n",
        "# (0,0): curves\n",
        "ax = axes[0, 0]\n",
        "ax.plot(xs, exact, label=\"Exact (cos x)\")\n",
        "ax.plot(xs, fwd, marker=\"o\", linestyle=\"None\", label=\"Forward\")\n",
        "ax.plot(xs, bwd, marker=\"s\", linestyle=\"None\", label=\"Backward\")\n",
        "ax.plot(xs, cen, marker=\"^\", linestyle=\"None\", label=\"Central\")\n",
        "ax.set_xlabel(\"x\"); ax.set_ylabel(\"Derivative\"); ax.set_title(\"Analytical vs numerical\")\n",
        "ax.legend()\n",
        "\n",
        "# (0,1): forward error\n",
        "ax = axes[0, 1]\n",
        "ax.plot(xs, ef, marker=\".\", linestyle=\"--\", label=\"Forward\")\n",
        "ax.set_xlabel(\"x\"); ax.set_ylabel(\"Absolute error\"); ax.set_title(\"Error (Forward)\")\n",
        "ax.legend()\n",
        "\n",
        "# (1,0): backward error\n",
        "ax = axes[1, 0]\n",
        "ax.plot(xs, eb, marker=\".\", linestyle=\"--\", label=\"Backward\")\n",
        "ax.set_xlabel(\"x\"); ax.set_ylabel(\"Absolute error\"); ax.set_title(\"Error (Backward)\")\n",
        "ax.legend()\n",
        "\n",
        "# (1,1): central error\n",
        "ax = axes[1, 1]\n",
        "ax.plot(xs, ec, marker=\".\", linestyle=\"--\", label=\"Central\")\n",
        "ax.set_xlabel(\"x\"); ax.set_ylabel(\"Absolute error\"); ax.set_title(\"Error (Central)\")\n",
        "ax.legend()\n",
        "\n",
        "fig.savefig(\"derivatives_and_errors_grid.jpg\", dpi=160)\n",
        "plt.show()\n",
        "\n",
        "def summarize_domain_errors(xs, err):\n",
        "    e = err.copy()\n",
        "    mask = ~np.isnan(e)\n",
        "    if not np.any(mask): return None\n",
        "    e = e[mask]; x = xs[mask]\n",
        "    i_max = int(np.argmax(e)); i_min = int(np.argmin(e))\n",
        "    return {\"max_error\": float(e[i_max]), \"x_at_max\": float(x[i_max]),\n",
        "            \"min_error\": float(e[i_min]), \"x_at_min\": float(x[i_min])}\n",
        "\n",
        "sum_f = summarize_domain_errors(xs, ef)\n",
        "sum_b = summarize_domain_errors(xs, eb)\n",
        "sum_c = summarize_domain_errors(xs, ec)\n",
        "sum_f, sum_b, sum_c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda369fc",
      "metadata": {
        "id": "dda369fc"
      },
      "source": [
        "## 4) Newton–Raphson roots on (0, 4π) — exact vs FD derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af3439f",
      "metadata": {
        "id": "9af3439f"
      },
      "outputs": [],
      "source": [
        "def newton_exact(x0, tol=1e-5, maxiter=100):\n",
        "    x = float(x0)\n",
        "    for k in range(maxiter):\n",
        "        fx = '''TODO Define exact function'''; dfx = '''TODO Define exact function derivative'''\n",
        "        if abs(dfx) < 1e-14: return None, k, \"derivative ~ 0\"\n",
        "        xnew = ## TODO write the function\n",
        "        if abs(xnew - x) < tol: return xnew, k+1, \"ok\"\n",
        "        x = ## TODO write the function\n",
        "    return x, maxiter, \"maxiter\"\n",
        "\n",
        "def newton_fd(x0, h, method=\"central\", tol=1e-5, maxiter=100):\n",
        "    x = float(x0)\n",
        "    for k in range(maxiter):\n",
        "        fx = math.sin(x)\n",
        "        if method == \"forward\":\n",
        "            dfx = ## TODO write the expression for the backward finite difference method\n",
        "        elif method == \"backward\":\n",
        "            dfx = ## TODO write the expression for the backward finite difference method\n",
        "        else:\n",
        "            dfx = ## TODO write the expression for the backward finite difference method\n",
        "        if abs(dfx) < 1e-14: return None, k, \"derivative ~ 0\"\n",
        "        xnew = ## TODO write the function\n",
        "        if abs(xnew - x) < tol: return xnew, k+1, \"ok\"\n",
        "        x = ## TODO write the function\n",
        "    return x, maxiter, \"maxiter\"\n",
        "\n",
        "a_nr, b_nr = 0.0, 4.0*math.pi\n",
        "records = []\n",
        "for x0 in INITIAL_GUESSES:\n",
        "    if not (a_nr <= x0 <= b_nr): continue\n",
        "    root_e, it_e, status_e = newton_exact(x0)\n",
        "    root_cf, it_cf, status_cf = newton_fd(x0, H_NEWTON, \"forward\")\n",
        "    root_cb, it_cb, status_cb = newton_fd(x0, H_NEWTON, \"backward\")\n",
        "    root_cc, it_cc, status_cc = newton_fd(x0, H_NEWTON, \"central\")\n",
        "    records.append({\n",
        "        \"x0\": x0,\n",
        "        \"exact_root\": root_e, \"exact_iters\": it_e, \"exact_status\": status_e,\n",
        "        \"fd_forward_root\": root_cf, \"fd_forward_iters\": it_cf, \"fd_forward_status\": status_cf,\n",
        "        \"fd_backward_root\": root_cb, \"fd_backward_iters\": it_cb, \"fd_backward_status\": status_cb,\n",
        "        \"fd_central_root\": root_cc, \"fd_central_iters\": it_cc, \"fd_central_status\": status_cc,\n",
        "    })\n",
        "\n",
        "pd.DataFrame.from_records(records)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}